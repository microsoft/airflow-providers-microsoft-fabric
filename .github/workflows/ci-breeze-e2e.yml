name: Run Airflow Integration Tests

on:
  pull_request:
    branches: ["**"]
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC

env:
  DAG_DEST_PATH: /opt/airflow/dags
  FABRIC_WORKSPACE_ID: your-workspace-id-here
  FABRIC_ITEM_ID: your-item-id-here

jobs:
  integration-test:
    runs-on: ubuntu-latest-4-cores  # 16GB RAM instead of 7GB
    strategy:
      matrix:
        include:
          - python-version: "3.12"
            airflow-version: "2.10.5"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Free up disk space and memory
      run: |
        # Remove unnecessary packages to free up memory and disk space
        sudo apt-get remove -y '^dotnet-.*' '^llvm-.*' '^mongodb-.*' '^mysql-.*' '^postgresql-.*'
        sudo apt-get autoremove -y
        sudo apt-get autoclean
        # Clear some caches
        sudo rm -rf /usr/local/lib/android
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /opt/ghc
        sudo rm -rf /usr/local/share/boost
        # Show memory info
        free -h
        df -h

    - name: Pull Airflow production image from Docker Hub
      run: |
        docker pull apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }}

    - name: Start Airflow container (with adequate memory)
      run: |
        docker run -d --memory=8g --memory-swap=8g --name airflow_ci \
          -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
          -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////home/airflow/airflow.db \
          -e AIRFLOW_HOME=/home/airflow \
          -e AIRFLOW__WEBSERVER__WORKERS=2 \
          -e AIRFLOW__SCHEDULER__PARSING_PROCESSES=2 \
          -e AIRFLOW__LOGGING__LOGGING_LEVEL=INFO \
          -e AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True \
          -e AIRFLOW__CORE__STORE_DAG_CODE=True \
          apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }} sleep infinity
        
        # Check memory limits
        docker exec airflow_ci sh -c '
          if [ -f /sys/fs/cgroup/memory.max ]; then
            echo "Cgroup v2 detected:"
            echo "memory.max: $(cat /sys/fs/cgroup/memory.max)"
            echo "memory.current: $(cat /sys/fs/cgroup/memory.current)"
          elif [ -f /sys/fs/cgroup/memory/memory.limit_in_bytes ]; then
            echo "Cgroup v1 detected:"
            echo "memory.limit_in_bytes: $(cat /sys/fs/cgroup/memory/memory.limit_in_bytes)"
          else
            echo "Could not detect memory limits"
          fi
        '

    - name: Build provider wheel locally
      run: |
        pip install build
        python -m build -o dist_local

    - name: Copy provider wheel into container
      run: |
        docker exec airflow_ci mkdir -p /tmp/wheels
        docker cp dist_local/. airflow_ci:/tmp/wheels/

    - name: Install provider wheel in container
      run: |
        docker exec airflow_ci bash -c "pip install --no-cache-dir /tmp/wheels/apache_airflow_providers_microsoft_fabric-*.whl"

    - name: Parse Fabric credentials from JSON secret
      id: fabric_auth
      run: |
        echo '${{ secrets.FABRIC_AUTH_JSON }}' > fabric.json
        echo "client_id=$(jq -r .client_id fabric.json)" >> $GITHUB_OUTPUT
        echo "tenant_id=$(jq -r .tenant_id fabric.json)" >> $GITHUB_OUTPUT
        echo "client_secret=$(jq -r .client_secret fabric.json)" >> $GITHUB_OUTPUT

    - name: Initialize Airflow database
      run: |
        docker exec airflow_ci airflow db init

    - name: Start Airflow components with staggered startup
      run: |
        # Create logs directory
        docker exec airflow_ci mkdir -p /home/airflow/logs
        
        # Start scheduler first and wait
        echo "Starting Airflow scheduler..."
        docker exec -d airflow_ci sh -c 'airflow scheduler > /home/airflow/logs/scheduler.log 2>&1'
        sleep 30
        
        # Check if scheduler is running
        if ! docker exec airflow_ci pgrep -f "airflow scheduler" > /dev/null; then
          echo "Scheduler failed to start, checking logs..."
          docker exec airflow_ci cat /home/airflow/logs/scheduler.log
          exit 1
        fi
        
        # Start webserver with reduced workers
        echo "Starting Airflow webserver..."
        docker exec -d airflow_ci sh -c 'airflow webserver --port 8080 > /home/airflow/logs/webserver.log 2>&1'
        sleep 20
        
        # Start triggerer
        echo "Starting Airflow triggerer..."
        docker exec -d airflow_ci sh -c 'airflow triggerer > /home/airflow/logs/triggerer.log 2>&1'
        sleep 10
        
        # Verify all components are running
        echo "Checking Airflow processes..."
        docker exec airflow_ci ps aux | grep airflow

    - name: Set Fabric connection in Airflow
      run: |
        docker exec -u airflow airflow_ci airflow connections add 'fabric_integration' \
          --conn-type 'microsoft-fabric' \
          --conn-extra '{
            "client_id": "${{ steps.fabric_auth.outputs.client_id }}",
            "tenant_id": "${{ steps.fabric_auth.outputs.tenant_id }}",
            "client_secret": "${{ steps.fabric_auth.outputs.client_secret }}"
          }'

    - name: Deploy all integration test DAGs to Airflow
      run: |
        docker exec airflow_ci mkdir -p $DAG_DEST_PATH
        find provider/tests/system -name "*.py" -exec docker cp {} airflow_ci:$DAG_DEST_PATH/ \;

    - name: Wait for DAG to be parsed
      run: |
        echo "Waiting for DAG to be parsed..."
        for i in {1..30}; do
          if docker exec airflow_ci airflow dags list | grep -q "test_fabric_notebook_run"; then
            echo "DAG found after $i attempts"
            break
          fi
          echo "Attempt $i: DAG not found yet, waiting..."
          sleep 10
        done
        docker exec airflow_ci airflow dags list

    - name: Run integration test DAG
      run: |
        docker exec airflow_ci airflow dags trigger test_fabric_notebook_run
        sleep 60
        docker exec airflow_ci airflow tasks list test_fabric_notebook_run
        docker exec airflow_ci airflow tasks test test_fabric_notebook_run run_notebook "$(date +%Y-%m-%d)"

    - name: Save Airflow logs
      if: always()
      run: |
        docker cp airflow_ci:/home/airflow/logs ./logs
        tar -czf logs.tar.gz logs

    - name: Upload logs artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: airflow-logs-${{ matrix.python-version }}-airflow-${{ matrix.airflow-version }}
        path: logs.tar.gz

    - name: Stop and remove container
      if: always()
      run: |
        docker stop airflow_ci
        docker rm airflow_ci