name: Run Airflow Integration Tests

on:
  pull_request:
    branches: ["**"]
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC

env:
  FABRIC_TENANT_ID: 249bbd2f-29a0-4a32-a667-ce59e2aee561
  FABRIC_CLIENT_ID: 9abcc0ae-393f-42ae-b46e-8f4dff217fd9
  # Visit github secret (CI_FABRIC_AUTH_SECRET) information: https://github.com/microsoft/airflow-providers-microsoft-fabric/settings/secrets/actions
  # TODO: Workspace and ItemID are still hardcoded in the DAG file
  FABRIC_WORKSPACE_ID: 988a1272-9da5-4936-be68-39e9b62d85ef
  FABRIC_ITEM_ID: 38579073-b94d-4b88-86e5-898bc15a2542
  AIRFLOW_HOME: /home/airflow
    # if not specified should fallback to https://api.fabric.microsoft.com
  FABRIC_API_ENDPOINT:
  AIRFLOW_URL: http://localhost:8080
  USER_NAME: admin
  PASSWORD: admin
  HEALTH_CHECK_TIMEOUT_MIN: 2
  DAG_RUN_TIMEOUT_MIN: 10

jobs:
  E2E-Integration-Tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      matrix:
        include:
          - python-version: "3.12"
            airflow-version: "2.10.5"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check initial memory state (host)
      run: |
        echo "=== INITIAL MEMORY STATE ON HOST ==="
        free -h
        df -h

    - name: Pull Airflow production image from Docker Hub
      run: |
        docker pull apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }}

    - name: Start Airflow container (basic setup)
      run: |
        echo "=== HOST MEMORY BEFORE CONTAINER START ==="
        free -h
        df -h

        docker run -d --memory=8g --memory-swap=8g --name airflow_ci \
          -p 8080:8080 \
          -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
          -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:///$AIRFLOW_HOME/airflow.db \
          -e AIRFLOW_HOME=$AIRFLOW_HOME \
          -e AIRFLOW__WEBSERVER__WORKERS=1 \
          -e AIRFLOW__SCHEDULER__PARSING_PROCESSES=1 \
          -e AIRFLOW__LOGGING__LOGGING_LEVEL=INFO \
          -e AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True \
          -e AIRFLOW__CORE__STORE_DAG_CODE=True \
          -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor \
          -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \
          -e AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10 \
          -e AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=False \
          -e AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth \
          apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }} \
          bash -c "while true; do sleep 30; done"

        echo "=== WAITING FOR CONTAINER TO START ==="
        sleep 10

        # Verify container is running
        if ! docker ps | grep -q airflow_ci; then
          echo "Container failed to start. Checking logs..."
          docker logs airflow_ci
          exit 1
        fi

        echo "=== CONTAINER MEMORY USAGE ==="
        docker stats --no-stream airflow_ci

    - name: Build provider wheel locally
      run: |
        pip install build
        python -m build -o dist_local

    - name: Copy provider wheel into container
      run: |
        docker exec airflow_ci mkdir -p /tmp/wheels
        docker cp dist_local/. airflow_ci:/tmp/wheels/

    - name: Install provider wheel in container
      run: |
        docker exec airflow_ci bash -c "pip install --no-cache-dir /tmp/wheels/apache_airflow_providers_microsoft_fabric-*.whl"
        docker stats --no-stream airflow_ci

    - name: Copy integration test DAGs to Airflow container
      run: |
        echo "Copying integration test DAGs to container..."
        docker exec airflow_ci bash -c "mkdir -p $AIRFLOW_HOME/dags"
        docker cp tests/system/dags/. airflow_ci:"$AIRFLOW_HOME/dags"

    - name: Fetch SPN Information used to create the Fabric connection
      id: fabric_auth
      run: |
        echo "tenant_id=${{ env.FABRIC_TENANT_ID }}" >> $GITHUB_OUTPUT      
        echo "client_id=${{ env.FABRIC_CLIENT_ID }}" >> $GITHUB_OUTPUT
        echo "client_secret=${{ secrets.CI_FABRIC_AUTH_SECRET }}" >> $GITHUB_OUTPUT

    - name: Initialize Airflow database
      run: |
        echo "=== INITIALIZING AIRFLOW DATABASE ==="
        docker exec airflow_ci airflow db init

        echo "=== CREATING AIRFLOW USER ==="
        docker exec airflow_ci airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || echo "User creation failed or user already exists"

    - name: Start Airflow components with staggered startup
      run: |
        echo "=== STARTING AIRFLOW COMPONENTS ==="
        # Start scheduler in background and get its PID
        echo "Starting scheduler..."
        docker exec -d airflow_ci bash -c "cd $AIRFLOW_HOME && airflow scheduler >> \$AIRFLOW_HOME/logs/scheduler.log 2>&1"
        
        # Start webserver in background 
        echo "Starting webserver..."
        docker exec -d airflow_ci bash -c "cd $AIRFLOW_HOME && airflow webserver --port 8080 >> \$AIRFLOW_HOME/logs/webserver.log 2>&1"
        
        # Start triggerer in background
        echo "Starting triggerer..."
        docker exec -d airflow_ci bash -c "cd $AIRFLOW_HOME && airflow triggerer >> \$AIRFLOW_HOME/logs/triggerer.log 2>&1"

    - name: Wait for health
      uses: ./.github/actions/check-airflow-health
      with:
        airflow_url:  ${{ env.AIRFLOW_URL }}
        username:     ${{ env.USER_NAME }}
        password:     ${{ env.PASSWORD }}
        timeout_minutes: ${{ env.HEALTH_CHECK_TIMEOUT_MIN }}

    - name: Set Fabric connection in Airflow
      run: |
        echo "=== CHECK FABRIC PROVIDER WAS INSTALLED ==="
        docker exec airflow_ci airflow providers list | grep microsoft 

        echo "=== SETTING FABRIC CONNECTION IN AIRFLOW ==="
        docker exec airflow_ci airflow connections add 'fabric_integration' \
          --conn-type 'microsoft-fabric' \
          --conn-extra '{
            "endpoint": "${{ env.FABRIC_API_ENDPOINT }}",
            "clientId": "${{ steps.fabric_auth.outputs.client_id }}",
            "tenantId": "${{ steps.fabric_auth.outputs.tenant_id }}",
            "clientSecret": "${{ steps.fabric_auth.outputs.client_secret }}"
          }'

        echo "=== CHECK CONNECTION IS AVAILABLE ==="
        docker exec airflow_ci airflow connections get 'fabric_integration'

    - name: Run DAG and wait for completion
      uses: ./.github/actions/run-airflow-dag
      with:
        airflow_url:     ${{ env.AIRFLOW_URL }}
        username:        ${{ env.USER_NAME }}
        password:        ${{ env.PASSWORD }}
        timeout_minutes: ${{ env.DAG_RUN_TIMEOUT_MIN }}
        dag_id:          test_fabric_notebook_run

    - name: Tar selected AIRFLOW_HOME subdirectories
      if: always()
      run: |
        docker exec airflow_ci bash -c "
            tar -czf /tmp/airflow_home.tar.gz \
              \"\$AIRFLOW_HOME/logs\" \
              \"\$AIRFLOW_HOME/dags\" \
              \"\$AIRFLOW_HOME/airflow.cfg\"
          "

    - name: Copy TAR archive from container
      if: always()
      run: |
        docker cp airflow_ci:/tmp/airflow_home.tar.gz ./airflow_home.tar.gz

    - name: Upload TAR artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: airflow-home-${{ matrix.python-version }}-airflow-${{ matrix.airflow-version }}
        path: airflow_home.tar.gz

    - name: Stop and remove container
      if: always()
      run: |
        docker stop airflow_ci || echo "Container already stopped"
        docker rm airflow_ci || echo "Container already removed"