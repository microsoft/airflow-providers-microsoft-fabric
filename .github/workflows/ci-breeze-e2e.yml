name: Run Airflow Integration Tests

on:
  pull_request:
    branches: ["**"]
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC

env:
  DAG_DEST_PATH: /opt/airflow/dags
  FABRIC_WORKSPACE_ID: your-workspace-id-here
  FABRIC_ITEM_ID: your-item-id-here

jobs:
  integration-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - python-version: "3.12"
            airflow-version: "2.10.5"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check initial memory state (host)
      run: |
        echo "=== INITIAL MEMORY STATE ON HOST ==="
        free -h
        df -h

    - name: Pull Airflow production image from Docker Hub
      run: |
        docker pull apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }}

    - name: Start Airflow container (basic setup)
      run: |
        echo "=== HOST MEMORY BEFORE CONTAINER START ==="
        free -h
        df -h

        # Start container with basic setup - just keep it running
        # Use SequentialExecutor which works with SQLite
        docker run -d --memory=8g --memory-swap=8g --name airflow_ci \
          -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
          -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////home/airflow/airflow.db \
          -e AIRFLOW_HOME=/home/airflow \
          -e AIRFLOW__WEBSERVER__WORKERS=1 \
          -e AIRFLOW__SCHEDULER__PARSING_PROCESSES=1 \
          -e AIRFLOW__LOGGING__LOGGING_LEVEL=INFO \
          -e AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True \
          -e AIRFLOW__CORE__STORE_DAG_CODE=True \
          -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor \
          -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \
          -e AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10 \
          -e AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=False \
          apache/airflow:${{ matrix.airflow-version }}-python${{ matrix.python-version }} \
          bash -c "while true; do sleep 30; done"

        echo "=== WAITING FOR CONTAINER TO START ==="
        sleep 10

        # Verify container is running
        if ! docker ps | grep -q airflow_ci; then
          echo "Container failed to start. Checking logs..."
          docker logs airflow_ci
          exit 1
        fi

        echo "=== CONTAINER MEMORY USAGE ==="
        docker stats --no-stream airflow_ci

    - name: Build provider wheel locally
      run: |
        pip install build
        python -m build -o dist_local

    - name: Copy provider wheel into container
      run: |
        docker exec airflow_ci mkdir -p /tmp/wheels
        docker cp dist_local/. airflow_ci:/tmp/wheels/

    - name: Install provider wheel in container
      run: |
        docker exec airflow_ci bash -c "pip install --no-cache-dir /tmp/wheels/apache_airflow_providers_microsoft_fabric-*.whl"
        docker stats --no-stream airflow_ci

    - name: Initialize Airflow database
      run: |
        echo "=== INITIALIZING AIRFLOW DATABASE ==="
        docker exec airflow_ci airflow db init
        
        echo "=== CREATING AIRFLOW USER ==="
        docker exec airflow_ci airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || echo "User creation failed or user already exists"

    - name: Parse Fabric credentials from JSON secret
      id: fabric_auth
      run: |
        echo '${{ secrets.FABRIC_AUTH_JSON }}' > fabric.json
        echo "client_id=$(jq -r .client_id fabric.json)" >> $GITHUB_OUTPUT
        echo "tenant_id=$(jq -r .tenant_id fabric.json)" >> $GITHUB_OUTPUT
        echo "client_secret=$(jq -r .client_secret fabric.json)" >> $GITHUB_OUTPUT

    - name: Start Airflow components with staggered startup
      run: |
        echo "=== STARTING AIRFLOW COMPONENTS ==="
        
        # Start scheduler in background and get its PID
        echo "Starting scheduler..."
        docker exec -d airflow_ci bash -c 'cd /home/airflow && airflow scheduler' 
        
        # Start webserver in background 
        echo "Starting webserver..."
        docker exec -d airflow_ci bash -c 'cd /home/airflow && airflow webserver --port 8080'
        
        # Start triggerer in background
        echo "Starting triggerer..."
        docker exec -d airflow_ci bash -c 'cd /home/airflow && airflow triggerer'
       
        echo "=== CHECKING AIRFLOW PROCESSES ==="
        sleep 15

        # Use /proc to check processes since ps isn't available
        docker exec airflow_ci ls -la /proc/*/cmdline 2>/dev/null | wc -l || echo "Cannot count processes"
        
        echo "=== CHECKING FOR AIRFLOW PROCESSES ==="
        docker exec airflow_ci bash -c 'find /proc -name cmdline -exec grep -l "airflow" {} \; 2>/dev/null | head -10' || echo "No airflow processes found"
        
        echo "=== CHECKING PORTS (using /proc/net/tcp) ==="
        docker exec airflow_ci cat /proc/net/tcp 2>/dev/null | grep -E "(1F90|0050)" || echo "Cannot check ports via /proc"
        
        echo "=== MEMORY STATUS ==="
        docker stats --no-stream airflow_ci
        
        # Test if webserver is responding using python instead of curl
        echo "=== TESTING WEBSERVER HEALTH ==="
        docker exec airflow_ci bash -c 'cat > /tmp/health_check.py << EOF
import urllib.request
import urllib.error
try:
    response = urllib.request.urlopen("http://localhost:8080/health", timeout=10)
    print("Webserver health check: HTTP", response.status, response.reason)
except urllib.error.HTTPError as e:
    print("Webserver HTTP error:", e.code, e.reason)
except urllib.error.URLError as e:
    print("Webserver connection error:", str(e))
except Exception as e:
    print("Webserver health check failed:", str(e))
EOF
python3 /tmp/health_check.py' || echo "Health check script failed"

        echo "=== CHECKING AIRFLOW STATUS ==="
        # Use airflow CLI to check status
        docker exec airflow_ci airflow jobs check --hostname localhost || echo "Jobs check failed"

    - name: Set Fabric connection in Airflow
      run: |
        docker exec airflow_ci airflow connections add 'fabric_integration' \
          --conn-type 'microsoft-fabric' \
          --conn-extra '{
            "client_id": "${{ steps.fabric_auth.outputs.client_id }}",
            "tenant_id": "${{ steps.fabric_auth.outputs.tenant_id }}",
            "client_secret": "${{ steps.fabric_auth.outputs.client_secret }}"
          }'

    - name: Deploy all integration test DAGs to Airflow
      run: |
        docker exec airflow_ci mkdir -p $DAG_DEST_PATH
        find provider/tests/system -name "*.py" -exec docker cp {} airflow_ci:$DAG_DEST_PATH/ \;

    - name: Wait for DAG to be parsed
      run: |
        echo "Waiting for DAG to be parsed..."
        for i in {1..30}; do
          if docker exec airflow_ci airflow dags list | grep -q "test_fabric_notebook_run"; then
            echo "DAG found after $i attempts"
            break
          fi
          echo "Attempt $i: DAG not found yet, waiting..."
          sleep 10
        done
        
        echo "=== ALL DAGS ==="
        docker exec airflow_ci airflow dags list

    - name: Run integration test DAG
      run: |
        # Trigger the DAG
        docker exec airflow_ci airflow dags trigger test_fabric_notebook_run
        
        # Wait for DAG run to be created
        sleep 30
        
        # List tasks
        docker exec airflow_ci airflow tasks list test_fabric_notebook_run
        
        # Test the specific task
        docker exec airflow_ci airflow tasks test test_fabric_notebook_run run_notebook "$(date +%Y-%m-%d)"

    - name: Check DAG run status
      run: |
        echo "=== DAG RUNS ==="
        docker exec airflow_ci airflow dags list-runs -d test_fabric_notebook_run || echo "No DAG runs found"
        
        echo "=== TASK INSTANCES ==="
        docker exec airflow_ci airflow tasks list test_fabric_notebook_run || echo "No tasks found"

    - name: Save Airflow logs
      if: always()
      run: |
        echo "=== CAPTURING AIRFLOW LOGS ==="
        
        # Create logs directory
        mkdir -p ./logs
        
        # Get Airflow logs from default location
        docker exec airflow_ci find /home/airflow/logs -name "*.log" -type f 2>/dev/null | head -20 || echo "No log files found"
        
        # Copy logs if they exist
        docker cp airflow_ci:/home/airflow/logs ./logs/ 2>/dev/null || echo "Failed to copy logs directory"
        
        # Get scheduler logs from stdout/stderr if running
        docker logs airflow_ci > ./logs/container.log 2>&1 || echo "Failed to get container logs"
        
        # Get current process status using /proc instead of ps
        docker exec airflow_ci bash -c 'find /proc -maxdepth 2 -name cmdline -exec bash -c "echo -n \"PID: \$(basename \$(dirname {})): \"; cat {} 2>/dev/null || echo \"(no cmdline)\"" \; | grep -v "^$" | head -20' > ./logs/processes.log 2>&1 || echo "Failed to get process list"
        
        # Get Airflow version and config info  
        docker exec airflow_ci airflow version > ./logs/airflow_version.log 2>&1 || echo "Failed to get Airflow version"
        docker exec airflow_ci airflow config list > ./logs/airflow_config.log 2>&1 || echo "Failed to get Airflow config"
        
        # Get container environment
        docker exec airflow_ci env | grep AIRFLOW > ./logs/airflow_env.log 2>&1 || echo "Failed to get Airflow environment"
        
        # List what we captured
        echo "=== CAPTURED LOG FILES ==="
        find ./logs -type f -name "*.log" -exec echo "File: {} ($(wc -l < {} || echo 0) lines)" \;
        
        tar -czf logs.tar.gz logs

    - name: Upload logs artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: airflow-logs-${{ matrix.python-version }}-airflow-${{ matrix.airflow-version }}
        path: logs.tar.gz

    - name: Stop and remove container
      if: always()
      run: |
        docker stop airflow_ci || echo "Container already stopped"
        docker rm airflow_ci || echo "Container already removed"